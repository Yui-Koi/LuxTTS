# ZipVoice diffusion / flow-matching semantics

Author: Lumen (analysis requested by Kiwi)

## 1. High-level picture

This is **not** a DDPM ancestral sampler or an SDE solver. It is a **deterministic probability-flow–style sampler trained via flow matching** on a straight-line interpolation between Gaussian noise and data.

- Training path:  
  `x_t = t * x_1 + (1 - t) * x_0` with
  - `x_1 = features` (clean acoustic features)
  - `x_0 = noise` (i.i.d. Gaussian)
- Model target: a **time-independent velocity**
  
  ```python
  xt = features * t + noise * (1 - t)
  ut = features - noise   # (B, T, F)
  vt = forward_fm_decoder(..., t=t, xt=xt, ...)
  loss = E[ || vt[mask] - ut[mask] ||^2 ]
  ```

So the network is trained to output

> `v ≈ x_1 - x_0` (a constant vector field in t for each sample)

rather than ε, x₀, or v in the DDPM/EDM sense. This is a classic **flow-matching / straight-line transport** setup.

At sampling time, they reconstruct `(x_0_pred, x_1_pred)` from the current `x` and predicted `v`, and move along that line with a custom time schedule. There is **no stochastic term** in the solver; stochasticity comes only from the **initial Gaussian x₀**.

---

## 2. Training graph and parameterization

**Entry**: `ZipVoice.forward` in `zipvoice/models/zipvoice.py`.

Shapes (batch = B, time = T, feature dim = F = `feat_dim`):

- `features`: `(B, T, F)` — clean acoustic features
- `features_lens`: `(B,)` — lengths
- `noise`: `(B, T, F)` — Gaussian noise
- `t`: `(B, 1, 1)` — scalar per sample in (0, 1)

Key steps:

1. **Text conditioning**

   ```python
   text_condition, padding_mask = forward_text_train(tokens, features_lens)
   # text_condition: (B, T, F)
   # padding_mask:   (B, T)  # True = padding
   ```

   - `forward_text_embed` embeds token ids with `nn.Embedding` and passes them
     through a ZipFormer encoder (no time embedding here).
   - `forward_text_condition` uses averaged durations and `torch.gather` to align
     text embeddings to feature frames → `(B, T, F)`.

2. **Speech condition (masked features)**

   ```python
   speech_condition_mask = condition_time_mask(...)
   # shape: (B, T), True where masked

   speech_condition = torch.where(
       speech_condition_mask.unsqueeze(-1), 0, features
   )  # (B, T, F)
   ```

3. **Flow-matching state and target**

   ```python
   xt = features * t + noise * (1 - t)  # (B, T, F)
   ut = features - noise                # (B, T, F)
   ```

   This is a straight-line interpolation from noise at t=0 to data at t=1.

4. **Velocity prediction**

   ```python
   vt = forward_fm_decoder(
       t=t,
       xt=xt,
       text_condition=text_condition,
       speech_condition=speech_condition,
       padding_mask=padding_mask,
   )
   ```

   Inside `forward_fm_decoder`:

   ```python
   xt = torch.cat([xt, text_condition, speech_condition], dim=2)
   # xt: (B, T, 3F)

   # t comes in as (B,1,1) or scalar; squeezed to (B,) if needed
   vt = fm_decoder(x=xt, t=t, padding_mask=padding_mask)
   # vt: (B, T, F)
   ```

   - The **FM decoder** is `TTSZipformer` with `use_time_embed=True` and
     `time_embed_dim = time_embed_dim`; it receives t as a 1D tensor `(B,)`.
   - So the architecture is *velocity (v) parameterization with explicit t
     embedding*, not ε or x₀.

5. **Loss, masking, gradients**

   ```python
   loss_mask = speech_condition_mask & (~padding_mask)
   fm_loss = torch.mean((vt[loss_mask] - ut[loss_mask]) ** 2)
   ```

   - Loss is only computed on positions that are **inside real speech**, not on
     padded frames and not on regions where `speech_condition` is nonzero.
   - All of this runs under regular training mode (`requires_grad=True`).

**Summary of training semantics**

- Numerical object: **continuous-time straight-line transport** from noise to data.
- Parameterization: **velocity v = x₁ − x₀**, time-dependent via the network but
  target is time-independent.
- Time convention: `t ∈ [0, 1]`, with:
  - `t = 0`: pure noise (`x_t = x_0`)
  - `t = 1`: pure data (`x_t = x_1`)

---

## 3. Inference sampler: EulerSolver

**Entry**: `EulerSolver.sample` in `zipvoice/models/modules/solver.py`.

Inputs:

- `x`: `(B, T, F)` — initial state; in usage, this is sampled noise `x0 ~ N(0, I)`
- `text_condition`: `(B, T, F)`
- `speech_condition`: `(B, T, F)`
- `padding_mask`: `(B, T)`
- `num_step`: int, number of ODE steps
- `guidance_scale`: float or `(B, 1, 1)`
- `t_start`, `t_end`: floats, default 0.0 → 1.0
- `t_shift`: float in (0, 1], time-shift for schedule

Timesteps are generated by `get_time_steps`:

```python
timesteps = torch.linspace(t_start, t_end, num_step + 1, device=device)

if t_shift == 1.0:
    return timesteps

inv_s = 1.0 / t_shift
denom = torch.add(inv_s, timesteps, alpha=1.0 - inv_s)
return timesteps.div_(denom)
```

This implements:

> `t' = t_shift * t / (1 + (t_shift - 1) * t)`

- For `t_shift < 1`, `t' < t` for `t > 0`, so the mesh is shifted toward **smaller t**.
- Given the training convention (`t=0` noise, `t=1` data), this puts **more steps
  in low-SNR (noisy) region**.

### 3.1. Single step update

For each step `k = 0..num_step-1`:

```python
t_cur = timesteps[step]    # scalar tensor
t_next = timesteps[step+1] # scalar tensor

v = model(t=t_cur, x=x, ...)

# 1. Recover endpoints of the line (x0, x1) from current x and predicted v:
#    Flow-matching relation: x_t = (1 - t) * x0 + t * x1
#    Network predicts v = x1 - x0

x_1_pred = x + (1.0 - t_cur) * v
x_0_pred = x - t_cur * v

if step < num_step - 1:
    # 2. Anchor-based Probability Flow "update":
    x = (1.0 - t_next) * x_0_pred + t_next * x_1_pred
else:
    # Final step: snap to predicted clean data
    x = x_1_pred
```

Interpretation:

1. **Assumed model:**
   
   - At any t,
     
     ```
     x_t = (1 - t) * x0 + t * x1
     v := x1 - x0
     ```
   
   - Given the current x and predicted v at t = t_cur, we solve for x0 and x1:
     
     ```
     x = (1 - t_cur) * x0 + t_cur * x1
       = x0 + t_cur * v
     ⇒ x0 = x - t_cur * v
     ⇒ x1 = x0 + v = x + (1 - t_cur) * v
     ```

2. **Propagation:**

   If v were *constant in t* and the true dynamics were exactly that straight
   line, then for any new t' we would have the exact solution

   ```
   x(t') = (1 - t') * x0 + t' * x1
   ```

   That is precisely what they use for `t_next` in non-final steps. So **each
   step is not an Euler approximation of an ODE, but an exact projection to the
   line implied by (x, v) at the new time** under this constant-velocity
   straight-line assumption.

3. **Final step:**

   - They skip the interpolation and snap to `x_1_pred`, the model’s estimate of
     the clean data endpoint.
   - This makes the last step independent of `t_end`; as long as the last step
     runs, you end at the model’s inferred x₁.

### 3.2. Determinism and noise

- There is **no `torch.randn_like(x)` inside the solver**.
- The only randomness at inference is:
  
  ```python
  x0 = torch.randn(B, T, F, device=...)
  ```

- Given a fixed `x0`, model weights, and conditions, sampling is **deterministic**.

Numerically, this is closer to a **probability flow ODE with an exact flow-matching
step** than to a DDPM ancestral or an SDE sampler.

---

## 4. Classifier-free guidance semantics

There are **two distinct wrappers**:

1. `DiffusionModel` → used by `EulerSolver` for the non-distilled model.
2. `DistillDiffusionModel` → used by `DistillEulerSolver` and expects
   the distilled model to handle guidance internally.

### 4.1. `DiffusionModel.forward`

Signature:

```python
forward(
    t: torch.Tensor,             # scalar tensor
    x: torch.Tensor,             # (B, T, F)
    text_condition: torch.Tensor,# (B, T, F)
    speech_condition: torch.Tensor,# (B, T, F)
    padding_mask: Optional[torch.Tensor] = None, # (B, T)
    guidance_scale: float | Tensor = 0.0,
) -> torch.Tensor
```

Behavior:

- If `guidance_scale == 0` → **no duplication**, call model once:

  ```python
  return model_func(...)
  ```

- If `guidance_scale ≠ 0`:

  ```python
  assert t.dim() == 0  # scalar t only

  # Duplicate batch
  x = torch.cat([x] * 2, dim=0)              # (2B, T, F)
  padding_mask = torch.cat([padding_mask] * 2, dim=0)

  # Text condition: zero out uncond branch
  text_condition = torch.cat(
      [torch.zeros_like(text_condition), text_condition], dim=0
  )  # (2B, T, F)

  if t > 0.5:
      # High-t (near data): uncond branch has no speech condition
      speech_condition = torch.cat(
          [torch.zeros_like(speech_condition), speech_condition], dim=0
      )
  else:
      # Low-t (noisy): both branches share speech_condition; guidance doubled
      guidance_scale = guidance_scale * 2
      speech_condition = torch.cat(
          [speech_condition, speech_condition], dim=0
      )

  data_uncond, data_cond = model_func(...).chunk(2, dim=0)
  res = (1 + guidance_scale) * data_cond - guidance_scale * data_uncond
  return res
  ```

Readings:

- **Text CFG**: uncond branch always zeros `text_condition`.
- **Speech CFG**:
  - For `t > 0.5` (closer to data), uncond branch has **no speech condition**;
    both text and speech are zeroed there.
  - For `t ≤ 0.5` (more noise), both branches share the same `speech_condition`,
    and they **double the guidance scale**. This is an unusual but deliberate
    design: CFG is made stronger in noisy regimes, but only with respect to text;
    speech condition is treated as a “style prior” there.
- Output is the standard linear CFG combination in **velocity space**:

  ```
  v_guided = (1 + w) * v_cond - w * v_uncond
  ```

### 4.2. Distilled model path

`DistillDiffusionModel.forward` is simpler:

```python
if not torch.is_tensor(guidance_scale):
    guidance_scale = torch.tensor(guidance_scale, dtype=t.dtype, device=t.device)
return model_func(
    t=t,
    xt=x,
    text_condition=text_condition,
    speech_condition=speech_condition,
    padding_mask=padding_mask,
    guidance_scale=guidance_scale,
)
```

- Distilled model is expected to **internally handle CFG** given `guidance_scale`.
- `DistillEulerSolver` simply swaps the wrapper but uses the same `sample` logic
  (same time schedule and anchored updates).

---

## 5. How sampling is called (shapes, dtypes, devices, grad)

From `zipvoice/models/zipvoice.py`, `ZipVoice.sample`:

- Inputs:
  - `tokens`: list[list[int]] — target text
  - `prompt_tokens`: list[list[int]] — prompt transcription
  - `prompt_features`: `(B, T_prompt, F)` — prompt acoustic features
  - `prompt_features_lens`: `(B,)`

- Text path:
  - If `duration == "predict"` (default in inference):

    ```python
    text_condition, padding_mask = forward_text_inference_ratio_duration(
        tokens, prompt_tokens, prompt_features_lens, speed
    )
    # text_condition: (B, T_full, F)
    # padding_mask:   (B, T_full)
    ```

- Speech condition:

  ```python
  speech_condition = F.pad(prompt_features, (0, 0, 0, num_frames - T_prompt))
  speech_condition_mask = make_pad_mask(prompt_features_lens, num_frames)
  speech_condition = torch.where(
      speech_condition_mask.unsqueeze(-1),
      torch.zeros_like(speech_condition),
      speech_condition,
  )
  ```

- Initial state `x` is standard Gaussian:

  ```python
  x0 = torch.randn(B, num_frames, F, device=text_condition.device)
  x1 = solver.sample(
      x=x0,
      text_condition=text_condition,
      speech_condition=speech_condition,
      padding_mask=padding_mask,
      num_step=num_step,
      guidance_scale=guidance_scale,
      t_shift=t_shift,
  )
  ```

Dtypes and gradients:

- In `infer_zipvoice.py`, `main()` is decorated with `@torch.inference_mode()`, so:
  - Gradients are **globally disabled** during inference.
  - There is **no autocast** / AMP in this script; everything runs in the model’s
    checkpoint dtype (almost certainly `float32`).
- `x0` is created on `text_condition.device` → no device round-trips.
- `timesteps` are created on `x.device` in `EulerSolver.sample`.

---

## 6. Algorithmic vs systems “dumbness” (risk inventory, not fixes)

### 6.1. Algorithmic / numerical

1. **Not an Euler integrator despite the name**
   - The solver is a sequence of **exact line projections** under the model’s
     inferred `(x0_pred, x1_pred)`. Calling it `EulerSolver` is semantically
     misleading; any refactor that “turns it into real Euler” would be a
     **behavior change**, not a cleanup.

2. **Implicit assumption of constant velocity along t**
   - Training target is time-independent (`features - noise`). The network could
     still learn t-dependence, but the loss does not encourage any particular
     temporal structure.
   - The sampler’s reconstruction formulas assume the true generative path is the
     straight segment between a single `(x0, x1)` pair. If the network’s
     predictions violate that (v depends strongly on t), the inversion becomes an
     approximation; multi-step updates simply re-apply this approximation.

3. **Final-step behavior**
   - Final state is **always `x_1_pred`**, regardless of `t_end` choice.
   - Changing `num_step` while keeping the same time range and network weights
     will alter the sequence of `(x, v)` pairs used to infer `x1_pred`, so
     samples are not step-invariant; but the last algebraic step is a snap.

4. **Time-shift t′(t) and its interaction with training**
   - Training is conducted with (apparently) uniform t in [0,1] (not shown here
     but implied by usage and comments).
   - At inference, they use a **nonlinear monotone remapping** via `t_shift`. That
     changes the effective density of evaluation points along the interpolation.
   - If one were to change the schedule (e.g., fewer steps but different
     `t_shift`), this is a semantic change in how much effort is spent in low vs
     high SNR regimes.

5. **Classifier-free guidance quirks**

   - Below `t ≤ 0.5`:
     - `speech_condition` is **identical** in cond and uncond branches.
     - `guidance_scale` is **doubled**.
   - Above `t > 0.5`:
     - `speech_condition` is **zero** in uncond branch.
   
   Any refactor that “simplifies” CFG (e.g., always zeroing speech in uncond, or
   always sharing it, or removing the `t > 0.5` branch) would be a genuine
   behavioral change. The logic here is nuanced and tied to how prompt
   conditioning is intended to affect style vs content.

6. **Deterministic probability flow**

   - No stochastic term during integration → this is not an SDE sampler; there is
     no ancestral noise injection.
   - If we were to introduce extra noise at intermediate steps, that would
     significantly change the semantic behavior and is **not a safe perf tweak**.

### 6.2. Systems / PyTorch-level

(Just enumerated; not yet optimized.)

1. **Per-step Python overhead**
   - `EulerSolver.sample` loops in Python over `num_step` and calls the model at
     each step; completely standard but potentially costly for large `num_step`.
   - Candidate optimizations (subject to preserving semantics):
     - fuse small ops around model call (x0/x1 reconstruction) into a single
       kernel or into model forward; 
     - reduce Python branching inside the guidance code.

2. **Repeated `torch.cat` and zero allocations for CFG**
   - For CFG, each step does:

     ```python
     x = torch.cat([x] * 2, dim=0)
     padding_mask = torch.cat([padding_mask] * 2, dim=0)
     text_condition = torch.cat([...])
     speech_condition = torch.cat([...])
     ```

   - These are new allocations every step; for long sequences, that’s nontrivial
     memory traffic and kernel overhead.
   - A more efficient layout would pre-allocate 2B tensors and use views / masks
     instead of re-concatenating.

3. **No AMP / mixed precision on inference path**
   - `infer_zipvoice.py` uses `@torch.inference_mode()` but not `autocast`, so
     on CUDA this runs in full fp32.
   - For large models, introducing AMP (carefully) is a pure systems optimization
     with negligible semantic risk, provided numerics remain stable.

4. **Lack of batched multi-sentence handling in raw-eval path**
   - `generate_sentence_raw_evaluation` runs a single sample at a time; that is
     fine for evaluation but suboptimal for large-scale synthesis.

5. **Timesteps tensor recomputed per sample**
   - `get_time_steps` is cheap but still done per call; if we introduce more
     granular control of stepping, we might want to cache or precompute
     schedules.

None of these systems issues require changing the mathematical method. We can
classify future refactors into:

- **Safe performance cleanups**: AMP/autocast, caching schedules, reducing
  concatenations, optimizing CFG batch handling, etc.
- **Numerical-method changes**: altering interpolation formulas, modifying
  CFG logic, introducing stochastic steps, or changing t-shift behavior.

Those categories should remain strictly separated in future work.
